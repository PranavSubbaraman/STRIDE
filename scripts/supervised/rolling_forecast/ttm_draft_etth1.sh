export CUDA_VISIBLE_DEVICES=0

model_name=ttm
token_num=7
token_len=96
seq_len=$[$token_num*$token_len]

# ETTh1 TTM checkpoint (fill this with your TTM run directory under ./checkpoints)
TTM_TEST_DIR="<SET_YOUR_TTM_CHECKPOINT_DIR>"
TTM_CKPT="checkpoint.pth"

# Testing TTM model (evaluation only)
for test_pred_len in 336
do
python -u run.py \
  --task_name forecast \
  --is_training 0 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --model_id ETTh1 \
  --model $model_name \
  --data MultivariateDatasetBenchmark  \
  --seq_len $seq_len \
  --input_token_len $token_len \
  --output_token_len $token_len \
  --test_seq_len $seq_len \
  --test_pred_len $test_pred_len \
  --batch_size 32 \
  --learning_rate 0.0001 \
  --d_model 1024 \
  --d_ff 2048 \
  --n_heads 8 \
  --e_layers 1 \
  --use_norm \
  --cosine \
  --test_dir "$TTM_TEST_DIR" \
  --test_file_name "$TTM_CKPT" \
  --n_vars 7 \
  --patch_size 96 \
  --stride 8 \
  --layers 2 \
  --hidden_dim 64 \
  --AP_levels 3 \
  --dropout 0.2 \
done

# Print per-batch timing
if [ -f result_inference_summary.txt ]; then
  echo "per_batch_s:"
  grep 'per_batch_s' result_inference_summary.txt | tail -n 1 || true
fi
